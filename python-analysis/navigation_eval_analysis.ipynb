{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Evaluation Analysis\n",
    "\n",
    "Comprehensive analysis of model performance on SF and CA navigation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for 20 SF models and 19 CA models\n"
     ]
    }
   ],
   "source": [
    "# Load the main evaluation results\n",
    "with open('/Users/hansenq/Documents/CS/nav-evals/data/eval-outputs/eval-results.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Add thinking model results manually (extracted from output files)\n",
    "thinking_results = {\n",
    "    'sf': {\n",
    "        'claude-3-7-sonnet-20250219--thinking': {\n",
    "            'totalSamples': 26, 'exactlyCorrect': 2, 'mostlyCorrect': 2,\n",
    "            'exactAccuracy': 0.077, 'mostlyAccuracy': 0.077, 'totalAccuracy': 0.154\n",
    "        },\n",
    "        'claude-opus-4-20250514--thinking': {\n",
    "            'totalSamples': 50, 'exactlyCorrect': 8, 'mostlyCorrect': 3,\n",
    "            'exactAccuracy': 0.16, 'mostlyAccuracy': 0.06, 'totalAccuracy': 0.22\n",
    "        },\n",
    "        'claude-sonnet-4-20250514--thinking': {\n",
    "            'totalSamples': 50, 'exactlyCorrect': 4, 'mostlyCorrect': 4,\n",
    "            'exactAccuracy': 0.08, 'mostlyAccuracy': 0.08, 'totalAccuracy': 0.16\n",
    "        }\n",
    "    },\n",
    "    'ca': {\n",
    "        'claude-3-7-sonnet-20250219--thinking': {\n",
    "            'totalSamples': 50, 'exactlyCorrect': 19, 'mostlyCorrect': 0,\n",
    "            'exactAccuracy': 0.38, 'mostlyAccuracy': 0.0, 'totalAccuracy': 0.38\n",
    "        },\n",
    "        'claude-opus-4-20250514--thinking': {\n",
    "            'totalSamples': 50, 'exactlyCorrect': 26, 'mostlyCorrect': 1,\n",
    "            'exactAccuracy': 0.52, 'mostlyAccuracy': 0.02, 'totalAccuracy': 0.54\n",
    "        },\n",
    "        'claude-sonnet-4-20250514--thinking': {\n",
    "            'totalSamples': 50, 'exactlyCorrect': 21, 'mostlyCorrect': 0,\n",
    "            'exactAccuracy': 0.42, 'mostlyAccuracy': 0.0, 'totalAccuracy': 0.42\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Merge thinking results into main data\n",
    "for dataset in ['sf', 'ca']:\n",
    "    eval_data[dataset].update(thinking_results[dataset])\n",
    "\n",
    "print(f\"Loaded data for {len(eval_data['sf'])} SF models and {len(eval_data['ca'])} CA models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o3-2025-04-16: o-series, openai, thinking=True\n",
      "claude-opus-4-20250514--thinking: claude, anthropic, thinking=True\n",
      "grok-4: grok, x-ai, thinking=True\n",
      "gemini-2_5-pro: gemini, google, thinking=True\n"
     ]
    }
   ],
   "source": [
    "# Define model categorization\n",
    "def categorize_model(model_name):\n",
    "    \"\"\"Categorize models by family, company, and thinking capability\"\"\"\n",
    "    model_name = model_name.lower()\n",
    "\n",
    "    # Determine if thinking model\n",
    "    is_thinking = any([\n",
    "        '--thinking' in model_name,\n",
    "        model_name.startswith('o3') or model_name.startswith('o4'),\n",
    "        'deepseek-r1' in model_name,\n",
    "        'grok' in model_name,\n",
    "        'gemini' in model_name  # Added gemini as thinking model\n",
    "    ])\n",
    "\n",
    "    # Determine family\n",
    "    if 'o3' in model_name or 'o4' in model_name:\n",
    "        family = 'o-series'\n",
    "    elif 'gpt' in model_name:\n",
    "        family = 'gpt'\n",
    "    elif 'claude' in model_name:\n",
    "        family = 'claude'\n",
    "    elif 'grok' in model_name:\n",
    "        family = 'grok'\n",
    "    elif 'deepseek' in model_name:\n",
    "        family = 'deepseek'\n",
    "    elif 'gemini' in model_name:\n",
    "        family = 'gemini'\n",
    "    elif 'llama' in model_name:\n",
    "        family = 'llama'\n",
    "    elif 'kimi' in model_name:\n",
    "        family = 'kimi'\n",
    "    else:\n",
    "        family = 'other'\n",
    "\n",
    "    # Determine company\n",
    "    if any(x in model_name for x in ['gpt', 'o3', 'o4']):\n",
    "        company = 'openai'\n",
    "    elif 'claude' in model_name:\n",
    "        company = 'anthropic'\n",
    "    elif 'gemini' in model_name:\n",
    "        company = 'google'\n",
    "    elif 'grok' in model_name:\n",
    "        company = 'x-ai'\n",
    "    elif 'deepseek' in model_name:\n",
    "        company = 'deepseek'\n",
    "    elif 'kimi' in model_name:\n",
    "        company = 'moonshot'\n",
    "    elif 'llama' in model_name:\n",
    "        company = 'meta'\n",
    "    else:\n",
    "        company = 'other'\n",
    "\n",
    "    return family, company, is_thinking\n",
    "\n",
    "# Test the categorization\n",
    "test_models = ['o3-2025-04-16', 'claude-opus-4-20250514--thinking', 'grok-4', 'gemini-2_5-pro']\n",
    "for model in test_models:\n",
    "    family, company, thinking = categorize_model(model)\n",
    "    print(f\"{model}: {family}, {company}, thinking={thinking}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Best Model Snapshots Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOP 10 MODELS BY EXACT ACCURACY ===\n",
      "\n",
      "SF Dataset:\n",
      "                               model  exact_accuracy   family  is_thinking\n",
      "                       o3-2025-04-16           0.220 o-series         True\n",
      "               google_gemini-2_5-pro           0.220   gemini         True\n",
      "                         x-ai_grok-4           0.200     grok         True\n",
      "    claude-opus-4-20250514--thinking           0.160   claude         True\n",
      "                  o4-mini-2025-04-16           0.100 o-series         True\n",
      "           deepseek_deepseek-r1-0528           0.100 deepseek         True\n",
      "                   gpt-4o-2024-11-20           0.080      gpt        False\n",
      "          claude-3-7-sonnet-20250219           0.080   claude        False\n",
      "  claude-sonnet-4-20250514--thinking           0.080   claude         True\n",
      "claude-3-7-sonnet-20250219--thinking           0.077   claude         True\n",
      "\n",
      "CA Dataset:\n",
      "                               model  exact_accuracy   family  is_thinking\n",
      "                         x-ai_grok-4            0.58     grok         True\n",
      "                  gpt-4_1-2025-04-14            0.56      gpt        False\n",
      "                       o3-2025-04-16            0.52 o-series         True\n",
      "    claude-opus-4-20250514--thinking            0.52   claude         True\n",
      "                   gpt-4o-2024-11-20            0.50      gpt        False\n",
      "               google_gemini-2_5-pro            0.46   gemini         True\n",
      "  claude-sonnet-4-20250514--thinking            0.42   claude         True\n",
      "           deepseek_deepseek-r1-0528            0.38 deepseek         True\n",
      "claude-3-7-sonnet-20250219--thinking            0.38   claude         True\n",
      "            claude-sonnet-4-20250514            0.36   claude        False\n",
      "\n",
      "=== TOP 10 MODELS BY MOSTLY-CORRECT ACCURACY ===\n",
      "\n",
      "SF Dataset:\n",
      "                               model  total_accuracy  mostly_accuracy   family  is_thinking\n",
      "                       o3-2025-04-16           0.300            0.080 o-series         True\n",
      "               google_gemini-2_5-pro           0.280            0.060   gemini         True\n",
      "                         x-ai_grok-4           0.220            0.020     grok         True\n",
      "    claude-opus-4-20250514--thinking           0.220            0.060   claude         True\n",
      "                   gpt-4o-2024-11-20           0.180            0.100      gpt        False\n",
      "  claude-sonnet-4-20250514--thinking           0.160            0.080   claude         True\n",
      "claude-3-7-sonnet-20250219--thinking           0.154            0.077   claude         True\n",
      "                  gpt-4_1-2025-04-14           0.140            0.100      gpt        False\n",
      "          claude-3-7-sonnet-20250219           0.140            0.060   claude        False\n",
      "   meta-llama_llama-3_1-70b-instruct           0.140            0.080    llama        False\n",
      "\n",
      "CA Dataset:\n",
      "                               model  total_accuracy  mostly_accuracy   family  is_thinking\n",
      "                       o3-2025-04-16            0.64             0.12 o-series         True\n",
      "                         x-ai_grok-4            0.62             0.04     grok         True\n",
      "                  gpt-4_1-2025-04-14            0.56             0.00      gpt        False\n",
      "    claude-opus-4-20250514--thinking            0.54             0.02   claude         True\n",
      "                   gpt-4o-2024-11-20            0.50             0.00      gpt        False\n",
      "               google_gemini-2_5-pro            0.48             0.02   gemini         True\n",
      "  claude-sonnet-4-20250514--thinking            0.42             0.00   claude         True\n",
      "           deepseek_deepseek-r1-0528            0.38             0.00 deepseek         True\n",
      "claude-3-7-sonnet-20250219--thinking            0.38             0.00   claude         True\n",
      "            claude-sonnet-4-20250514            0.36             0.00   claude        False\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive results dataframe\n",
    "results = []\n",
    "for dataset in ['sf', 'ca']:\n",
    "    for model, data in eval_data[dataset].items():\n",
    "        family, company, is_thinking = categorize_model(model)\n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'dataset': dataset,\n",
    "            'family': family,\n",
    "            'company': company,\n",
    "            'is_thinking': is_thinking,\n",
    "            'exact_accuracy': data['exactAccuracy'],\n",
    "            'mostly_accuracy': data['mostlyAccuracy'],\n",
    "            'total_accuracy': data.get('totalAccuracy', data['exactAccuracy'] + data['mostlyAccuracy']),\n",
    "            'total_samples': data['totalSamples']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Top performers for each dataset and metric\n",
    "print(\"=== TOP 10 MODELS BY EXACT ACCURACY ===\")\n",
    "print(\"\\nSF Dataset:\")\n",
    "sf_exact = df[df['dataset'] == 'sf'].nlargest(10, 'exact_accuracy')[['model', 'exact_accuracy', 'family', 'is_thinking']]\n",
    "print(sf_exact.to_string(index=False))\n",
    "\n",
    "print(\"\\nCA Dataset:\")\n",
    "ca_exact = df[df['dataset'] == 'ca'].nlargest(10, 'exact_accuracy')[['model', 'exact_accuracy', 'family', 'is_thinking']]\n",
    "print(ca_exact.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== TOP 10 MODELS BY MOSTLY-CORRECT ACCURACY ===\")\n",
    "print(\"\\nSF Dataset:\")\n",
    "sf_mostly = df[df['dataset'] == 'sf'].nlargest(10, 'total_accuracy')[['model', 'total_accuracy', 'mostly_accuracy', 'family', 'is_thinking']]\n",
    "print(sf_mostly.to_string(index=False))\n",
    "\n",
    "print(\"\\nCA Dataset:\")\n",
    "ca_mostly = df[df['dataset'] == 'ca'].nlargest(10, 'total_accuracy')[['model', 'total_accuracy', 'mostly_accuracy', 'family', 'is_thinking']]\n",
    "print(ca_mostly.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Family Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "np.int64(26)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 26",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get best result from each family\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m family_best = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfamily\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexact_accuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_accuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfamily\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_accuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43midxmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m.reset_index()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== BEST RESULT FROM EACH MODEL FAMILY ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33msf\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mca\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1432\u001b[39m, in \u001b[36mDataFrameGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = engine_kwargs\n\u001b[32m   1431\u001b[39m op = GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args=args, kwargs=kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m result = \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1434\u001b[39m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[32m   1435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.as_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/apply.py:190\u001b[39m, in \u001b[36mApply.agg\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_str()\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agg_list_like()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/apply.py:423\u001b[39m, in \u001b[36mApply.agg_dict_like\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame | Series:\n\u001b[32m    416\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m \u001b[33;03m    Result of aggregation.\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/apply.py:1603\u001b[39m, in \u001b[36mGroupByApply.agg_or_apply_dict_like\u001b[39m\u001b[34m(self, op_name)\u001b[39m\n\u001b[32m   1598\u001b[39m     kwargs.update({\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m: engine, \u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m: engine_kwargs})\n\u001b[32m   1600\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m com.temp_setattr(\n\u001b[32m   1601\u001b[39m     obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition=\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1602\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1603\u001b[39m     result_index, result_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1606\u001b[39m result = \u001b[38;5;28mself\u001b[39m.wrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[32m   1607\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/apply.py:496\u001b[39m, in \u001b[36mApply.compute_dict_like\u001b[39m\u001b[34m(self, op_name, selected_obj, selection, kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m         results += key_data\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    495\u001b[39m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     results = \u001b[43m[\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gotitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    500\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(func.keys())\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/apply.py:497\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    493\u001b[39m         results += key_data\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    495\u001b[39m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[32m    496\u001b[39m     results = [\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gotitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func.items()\n\u001b[32m    499\u001b[39m     ]\n\u001b[32m    500\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(func.keys())\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/groupby/generic.py:291\u001b[39m, in \u001b[36mSeriesGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._constructor(\n\u001b[32m    284\u001b[39m         [],\n\u001b[32m    285\u001b[39m         name=\u001b[38;5;28mself\u001b[39m.obj.name,\n\u001b[32m    286\u001b[39m         index=\u001b[38;5;28mself\u001b[39m._grouper.result_index,\n\u001b[32m    287\u001b[39m         dtype=obj.dtype,\n\u001b[32m    288\u001b[39m     )\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._grouper.nkeys > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_agg_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._python_agg_general(func, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/groupby/generic.py:327\u001b[39m, in \u001b[36mSeriesGroupBy._python_agg_general\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m f = \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, *args, **kwargs)\n\u001b[32m    326\u001b[39m obj = \u001b[38;5;28mself\u001b[39m._obj_with_exclusions\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m res = obj._constructor(result, name=obj.name)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wrap_aggregated_output(res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/groupby/ops.py:864\u001b[39m, in \u001b[36mBaseGrouper.agg_series\u001b[39m\u001b[34m(self, obj, func, preserve_dtype)\u001b[39m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj._values, np.ndarray):\n\u001b[32m    858\u001b[39m     \u001b[38;5;66;03m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[32m    862\u001b[39m     preserve_dtype = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m npvalues = lib.maybe_convert_objects(result, try_float=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    867\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/groupby/ops.py:885\u001b[39m, in \u001b[36mBaseGrouper._aggregate_series_pure_python\u001b[39m\u001b[34m(self, obj, func)\u001b[39m\n\u001b[32m    882\u001b[39m splitter = \u001b[38;5;28mself\u001b[39m._get_splitter(obj, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m     res = extract_result(res)\n\u001b[32m    888\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[32m    889\u001b[39m         \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/groupby/generic.py:324\u001b[39m, in \u001b[36mSeriesGroupBy._python_agg_general.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    322\u001b[39m     alias = com._builtin_table_alias[func]\n\u001b[32m    323\u001b[39m     warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, alias)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m f = \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m obj = \u001b[38;5;28mself\u001b[39m._obj_with_exclusions\n\u001b[32m    327\u001b[39m result = \u001b[38;5;28mself\u001b[39m._grouper.agg_series(obj, f)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get best result from each family\u001b[39;00m\n\u001b[32m      2\u001b[39m family_best = df.groupby([\u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfamily\u001b[39m\u001b[33m'\u001b[39m]).agg({\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mexact_accuracy\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_accuracy\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfamily\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_accuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43midxmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m }).reset_index()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== BEST RESULT FROM EACH MODEL FAMILY ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33msf\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mca\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1183\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1181\u001b[39m     key = \u001b[38;5;28mtuple\u001b[39m(com.apply_if_callable(x, \u001b[38;5;28mself\u001b[39m.obj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_tuple(key)\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1186\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4226\u001b[39m, in \u001b[36mDataFrame._get_value\u001b[39m\u001b[34m(self, index, col, takeable)\u001b[39m\n\u001b[32m   4220\u001b[39m engine = \u001b[38;5;28mself\u001b[39m.index._engine\n\u001b[32m   4222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, MultiIndex):\n\u001b[32m   4223\u001b[39m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[32m   4224\u001b[39m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[32m   4225\u001b[39m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4226\u001b[39m     row = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m series._values[row]\n\u001b[32m   4229\u001b[39m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[32m   4230\u001b[39m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS/nav-evals/python-analysis/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: np.int64(26)"
     ]
    }
   ],
   "source": [
    "# Get best result from each family\n",
    "family_best = df.groupby(['dataset', 'family']).agg({\n",
    "    'exact_accuracy': 'max',\n",
    "    'total_accuracy': 'max',\n",
    "    'model': lambda x: df.loc[df.groupby(['dataset', 'family'])['total_accuracy'].idxmax()].loc[x.index[0], 'model']\n",
    "}).reset_index()\n",
    "\n",
    "print(\"=== BEST RESULT FROM EACH MODEL FAMILY ===\")\n",
    "for dataset in ['sf', 'ca']:\n",
    "    print(f\"\\n{dataset.upper()} Dataset:\")\n",
    "    dataset_family = family_best[family_best['dataset'] == dataset].sort_values('total_accuracy', ascending=False)\n",
    "    print(dataset_family[['family', 'model', 'exact_accuracy', 'total_accuracy']].to_string(index=False))\n",
    "\n",
    "# Visualize family comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for i, dataset in enumerate(['sf', 'ca']):\n",
    "    data = family_best[family_best['dataset'] == dataset].sort_values('total_accuracy', ascending=True)\n",
    "    axes[i].barh(data['family'], data['total_accuracy'])\n",
    "    axes[i].set_title(f'{dataset.upper()} Dataset - Best Family Performance')\n",
    "    axes[i].set_xlabel('Total Accuracy (Mostly-Correct)')\n",
    "    axes[i].grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for j, v in enumerate(data['total_accuracy']):\n",
    "        axes[i].text(v + 0.01, j, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Thinking vs Non-Thinking Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare thinking vs non-thinking models\n",
    "thinking_comparison = df.groupby(['dataset', 'is_thinking']).agg({\n",
    "    'exact_accuracy': ['mean', 'max', 'std'],\n",
    "    'total_accuracy': ['mean', 'max', 'std'],\n",
    "    'model': 'count'\n",
    "}).round(3)\n",
    "\n",
    "print(\"=== THINKING vs NON-THINKING MODEL COMPARISON ===\")\n",
    "print(thinking_comparison)\n",
    "\n",
    "# Best thinking and non-thinking models\n",
    "print(\"\\n=== BEST THINKING MODELS ===\")\n",
    "thinking_models = df[df['is_thinking'] == True]\n",
    "for dataset in ['sf', 'ca']:\n",
    "    best_thinking = thinking_models[thinking_models['dataset'] == dataset].nlargest(3, 'total_accuracy')\n",
    "    print(f\"\\n{dataset.upper()} Dataset:\")\n",
    "    print(best_thinking[['model', 'exact_accuracy', 'total_accuracy', 'family']].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== BEST NON-THINKING MODELS ===\")\n",
    "non_thinking_models = df[df['is_thinking'] == False]\n",
    "for dataset in ['sf', 'ca']:\n",
    "    best_non_thinking = non_thinking_models[non_thinking_models['dataset'] == dataset].nlargest(3, 'total_accuracy')\n",
    "    print(f\"\\n{dataset.upper()} Dataset:\")\n",
    "    print(best_non_thinking[['model', 'exact_accuracy', 'total_accuracy', 'family']].to_string(index=False))\n",
    "\n",
    "# Visualize thinking vs non-thinking\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = [('exact_accuracy', 'Exact Accuracy'), ('total_accuracy', 'Total Accuracy (Mostly-Correct)')]\n",
    "datasets = ['sf', 'ca']\n",
    "\n",
    "for i, (metric, title) in enumerate(metrics):\n",
    "    for j, dataset in enumerate(datasets):\n",
    "        data = df[df['dataset'] == dataset]\n",
    "\n",
    "        thinking_data = data[data['is_thinking'] == True][metric]\n",
    "        non_thinking_data = data[data['is_thinking'] == False][metric]\n",
    "\n",
    "        axes[i,j].boxplot([thinking_data, non_thinking_data],\n",
    "                         labels=['Thinking', 'Non-Thinking'])\n",
    "        axes[i,j].set_title(f'{dataset.upper()} Dataset - {title}')\n",
    "        axes[i,j].set_ylabel('Accuracy')\n",
    "        axes[i,j].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Thinking Improvement Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate thinking improvement for models that have both versions\n",
    "thinking_improvements = []\n",
    "\n",
    "# Find models with both thinking and non-thinking versions\n",
    "for dataset in ['sf', 'ca']:\n",
    "    models_in_dataset = set(df[df['dataset'] == dataset]['model'].str.replace('--thinking', ''))\n",
    "\n",
    "    for base_model in models_in_dataset:\n",
    "        thinking_version = base_model + '--thinking'\n",
    "\n",
    "        # Check if both versions exist\n",
    "        base_data = df[(df['dataset'] == dataset) & (df['model'] == base_model)]\n",
    "        thinking_data = df[(df['dataset'] == dataset) & (df['model'] == thinking_version)]\n",
    "\n",
    "        if not base_data.empty and not thinking_data.empty:\n",
    "            base_acc = base_data.iloc[0]['total_accuracy']\n",
    "            thinking_acc = thinking_data.iloc[0]['total_accuracy']\n",
    "            improvement = thinking_acc - base_acc\n",
    "\n",
    "            thinking_improvements.append({\n",
    "                'dataset': dataset,\n",
    "                'base_model': base_model,\n",
    "                'base_accuracy': base_acc,\n",
    "                'thinking_accuracy': thinking_acc,\n",
    "                'improvement': improvement,\n",
    "                'relative_improvement': improvement / base_acc if base_acc > 0 else 0\n",
    "            })\n",
    "\n",
    "improvement_df = pd.DataFrame(thinking_improvements)\n",
    "\n",
    "if not improvement_df.empty:\n",
    "    print(\"=== THINKING MODEL IMPROVEMENTS ===\")\n",
    "    print(improvement_df.round(3).to_string(index=False))\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"\\n=== IMPROVEMENT SUMMARY ===\")\n",
    "    print(f\"Average absolute improvement: {improvement_df['improvement'].mean():.3f}\")\n",
    "    print(f\"Average relative improvement: {improvement_df['relative_improvement'].mean():.1%}\")\n",
    "    print(f\"Models improved: {(improvement_df['improvement'] > 0).sum()}/{len(improvement_df)}\")\n",
    "\n",
    "    # Visualize improvements\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    for i, dataset in enumerate(['sf', 'ca']):\n",
    "        data = improvement_df[improvement_df['dataset'] == dataset]\n",
    "        if not data.empty:\n",
    "            x_pos = range(len(data))\n",
    "            axes[i].bar(x_pos, data['improvement'],\n",
    "                       color=['green' if x > 0 else 'red' for x in data['improvement']])\n",
    "            axes[i].set_title(f'{dataset.upper()} Dataset - Thinking Model Improvements')\n",
    "            axes[i].set_xlabel('Models')\n",
    "            axes[i].set_ylabel('Accuracy Improvement')\n",
    "            axes[i].set_xticks(x_pos)\n",
    "            axes[i].set_xticklabels([m.replace('claude-', '') for m in data['base_model']], rotation=45)\n",
    "            axes[i].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "            axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models found with both thinking and non-thinking versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Company Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best result from each company\n",
    "company_best = df.groupby(['dataset', 'company']).agg({\n",
    "    'exact_accuracy': 'max',\n",
    "    'total_accuracy': 'max',\n",
    "    'model': lambda x: df.loc[df.groupby(['dataset', 'company'])['total_accuracy'].idxmax()].loc[x.index[0], 'model']\n",
    "}).reset_index()\n",
    "\n",
    "print(\"=== BEST RESULT FROM EACH COMPANY ===\")\n",
    "for dataset in ['sf', 'ca']:\n",
    "    print(f\"\\n{dataset.upper()} Dataset:\")\n",
    "    dataset_company = company_best[company_best['dataset'] == dataset].sort_values('total_accuracy', ascending=False)\n",
    "    print(dataset_company[['company', 'model', 'exact_accuracy', 'total_accuracy']].to_string(index=False))\n",
    "\n",
    "# Visualize company comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for i, dataset in enumerate(['sf', 'ca']):\n",
    "    data = company_best[company_best['dataset'] == dataset].sort_values('total_accuracy', ascending=True)\n",
    "    bars = axes[i].barh(data['company'], data['total_accuracy'])\n",
    "    axes[i].set_title(f'{dataset.upper()} Dataset - Best Company Performance')\n",
    "    axes[i].set_xlabel('Total Accuracy (Mostly-Correct)')\n",
    "    axes[i].grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for j, v in enumerate(data['total_accuracy']):\n",
    "        axes[i].text(v + 0.01, j, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SF vs CA Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare overall difficulty between datasets\n",
    "dataset_stats = df.groupby('dataset').agg({\n",
    "    'exact_accuracy': ['mean', 'median', 'std', 'max'],\n",
    "    'total_accuracy': ['mean', 'median', 'std', 'max'],\n",
    "    'model': 'count'\n",
    "}).round(3)\n",
    "\n",
    "print(\"=== SF vs CA DATASET DIFFICULTY COMPARISON ===\")\n",
    "print(dataset_stats)\n",
    "\n",
    "# Models that appear in both datasets\n",
    "sf_models = set(df[df['dataset'] == 'sf']['model'])\n",
    "ca_models = set(df[df['dataset'] == 'ca']['model'])\n",
    "common_models = sf_models.intersection(ca_models)\n",
    "\n",
    "print(f\"\\nModels evaluated on both datasets: {len(common_models)}\")\n",
    "\n",
    "# Compare performance on common models\n",
    "if common_models:\n",
    "    comparison_data = []\n",
    "    for model in common_models:\n",
    "        sf_perf = df[(df['dataset'] == 'sf') & (df['model'] == model)].iloc[0]\n",
    "        ca_perf = df[(df['dataset'] == 'ca') & (df['model'] == model)].iloc[0]\n",
    "\n",
    "        comparison_data.append({\n",
    "            'model': model,\n",
    "            'sf_exact': sf_perf['exact_accuracy'],\n",
    "            'ca_exact': ca_perf['exact_accuracy'],\n",
    "            'sf_total': sf_perf['total_accuracy'],\n",
    "            'ca_total': ca_perf['total_accuracy'],\n",
    "            'exact_diff': ca_perf['exact_accuracy'] - sf_perf['exact_accuracy'],\n",
    "            'total_diff': ca_perf['total_accuracy'] - sf_perf['total_accuracy']\n",
    "        })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "    print(\"\\n=== PERFORMANCE COMPARISON ON COMMON MODELS ===\")\n",
    "    print(comparison_df[['model', 'sf_total', 'ca_total', 'total_diff']].sort_values('total_diff', ascending=False).round(3).to_string(index=False))\n",
    "\n",
    "    print(f\"\\nAverage CA advantage: {comparison_df['total_diff'].mean():.3f}\")\n",
    "    print(f\"Models performing better on CA: {(comparison_df['total_diff'] > 0).sum()}/{len(comparison_df)}\")\n",
    "\n",
    "# Visualize dataset comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Distribution comparison\n",
    "sf_data = df[df['dataset'] == 'sf']['total_accuracy']\n",
    "ca_data = df[df['dataset'] == 'ca']['total_accuracy']\n",
    "\n",
    "axes[0,0].hist([sf_data, ca_data], bins=15, alpha=0.7, label=['SF', 'CA'])\n",
    "axes[0,0].set_title('Accuracy Distribution Comparison')\n",
    "axes[0,0].set_xlabel('Total Accuracy')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "axes[0,1].boxplot([sf_data, ca_data], labels=['SF', 'CA'])\n",
    "axes[0,1].set_title('Accuracy Distribution Box Plot')\n",
    "axes[0,1].set_ylabel('Total Accuracy')\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# Scatter plot for common models\n",
    "if not comparison_df.empty:\n",
    "    axes[1,0].scatter(comparison_df['sf_total'], comparison_df['ca_total'], alpha=0.7)\n",
    "    axes[1,0].plot([0, 0.7], [0, 0.7], 'r--', alpha=0.5, label='Equal Performance')\n",
    "    axes[1,0].set_xlabel('SF Accuracy')\n",
    "    axes[1,0].set_ylabel('CA Accuracy')\n",
    "    axes[1,0].set_title('SF vs CA Performance (Common Models)')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "# Improvement/degradation from SF to CA\n",
    "if not comparison_df.empty:\n",
    "    axes[1,1].bar(range(len(comparison_df)), comparison_df['total_diff'],\n",
    "                  color=['green' if x > 0 else 'red' for x in comparison_df['total_diff']])\n",
    "    axes[1,1].set_title('CA vs SF Performance Difference')\n",
    "    axes[1,1].set_xlabel('Models')\n",
    "    axes[1,1].set_ylabel('CA - SF Accuracy')\n",
    "    axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[1,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional Interesting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token usage analysis\n",
    "print(\"=== TOKEN USAGE ANALYSIS ===\")\n",
    "token_data = []\n",
    "for dataset in ['sf', 'ca']:\n",
    "    for model, data in eval_data[dataset].items():\n",
    "        if 'avgOutputTokens' in data:\n",
    "            family, company, is_thinking = categorize_model(model)\n",
    "            token_data.append({\n",
    "                'model': model,\n",
    "                'dataset': dataset,\n",
    "                'family': family,\n",
    "                'is_thinking': is_thinking,\n",
    "                'avg_tokens': data['avgOutputTokens'],\n",
    "                'total_accuracy': data.get('totalAccuracy', data['exactAccuracy'] + data['mostlyAccuracy'])\n",
    "            })\n",
    "\n",
    "token_df = pd.DataFrame(token_data)\n",
    "\n",
    "if not token_df.empty:\n",
    "    # Token usage by thinking models\n",
    "    thinking_tokens = token_df.groupby(['dataset', 'is_thinking'])['avg_tokens'].agg(['mean', 'median', 'std']).round(1)\n",
    "    print(\"\\nAverage token usage by thinking capability:\")\n",
    "    print(thinking_tokens)\n",
    "\n",
    "    # Efficiency: accuracy per token\n",
    "    token_df['efficiency'] = token_df['total_accuracy'] / token_df['avg_tokens'] * 1000  # accuracy per 1k tokens\n",
    "\n",
    "    print(\"\\n=== MOST EFFICIENT MODELS (Accuracy per 1K tokens) ===\")\n",
    "    for dataset in ['sf', 'ca']:\n",
    "        print(f\"\\n{dataset.upper()} Dataset:\")\n",
    "        efficient = token_df[token_df['dataset'] == dataset].nlargest(5, 'efficiency')\n",
    "        print(efficient[['model', 'total_accuracy', 'avg_tokens', 'efficiency']].round(3).to_string(index=False))\n",
    "\n",
    "# Model size/complexity analysis\n",
    "print(\"\\n\\n=== MODEL COMPLEXITY ANALYSIS ===\")\n",
    "complexity_order = {\n",
    "    'haiku': 1, 'mini': 2, 'sonnet': 3, 'gpt-4': 4, 'opus': 5,\n",
    "    'turbo': 4, 'o3': 6, 'o4': 7, 'grok': 5, 'gemini': 4\n",
    "}\n",
    "\n",
    "def get_complexity(model_name):\n",
    "    model_lower = model_name.lower()\n",
    "    for key, value in complexity_order.items():\n",
    "        if key in model_lower:\n",
    "            return value\n",
    "    return 3  # default\n",
    "\n",
    "df['complexity'] = df['model'].apply(get_complexity)\n",
    "complexity_perf = df.groupby(['dataset', 'complexity']).agg({\n",
    "    'total_accuracy': ['mean', 'max'],\n",
    "    'model': 'count'\n",
    "}).round(3)\n",
    "\n",
    "print(\"Performance by model complexity:\")\n",
    "print(complexity_perf)\n",
    "\n",
    "# Model release date analysis (approximate)\n",
    "print(\"\\n\\n=== PERFORMANCE BY MODEL GENERATION ===\")\n",
    "def get_generation(model_name):\n",
    "    if '2025' in model_name or 'o3' in model_name or 'o4' in model_name or 'grok-4' in model_name:\n",
    "        return '2025'\n",
    "    elif '2024' in model_name:\n",
    "        return '2024'\n",
    "    else:\n",
    "        return 'older'\n",
    "\n",
    "df['generation'] = df['model'].apply(get_generation)\n",
    "generation_perf = df.groupby(['dataset', 'generation']).agg({\n",
    "    'total_accuracy': ['mean', 'max', 'count'],\n",
    "    'exact_accuracy': ['mean', 'max']\n",
    "}).round(3)\n",
    "\n",
    "print(generation_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== KEY INSIGHTS SUMMARY ===\")\n",
    "print()\n",
    "\n",
    "# Best overall performers\n",
    "sf_best = df[df['dataset'] == 'sf'].nlargest(1, 'total_accuracy').iloc[0]\n",
    "ca_best = df[df['dataset'] == 'ca'].nlargest(1, 'total_accuracy').iloc[0]\n",
    "\n",
    "print(f\"1. BEST OVERALL MODELS:\")\n",
    "print(f\"   SF Dataset: {sf_best['model']} ({sf_best['total_accuracy']:.1%})\")\n",
    "print(f\"   CA Dataset: {ca_best['model']} ({ca_best['total_accuracy']:.1%})\")\n",
    "\n",
    "# Family winners\n",
    "print(f\"\\n2. BEST MODEL FAMILIES:\")\n",
    "for dataset in ['sf', 'ca']:\n",
    "    best_family = family_best[family_best['dataset'] == dataset].nlargest(1, 'total_accuracy').iloc[0]\n",
    "    print(f\"   {dataset.upper()}: {best_family['family']} ({best_family['total_accuracy']:.1%})\")\n",
    "\n",
    "# Company winners\n",
    "print(f\"\\n3. BEST COMPANIES:\")\n",
    "for dataset in ['sf', 'ca']:\n",
    "    best_company = company_best[company_best['dataset'] == dataset].nlargest(1, 'total_accuracy').iloc[0]\n",
    "    print(f\"   {dataset.upper()}: {best_company['company']} ({best_company['total_accuracy']:.1%})\")\n",
    "\n",
    "# Thinking advantage\n",
    "print(f\"\\n4. THINKING MODEL ADVANTAGE:\")\n",
    "for dataset in ['sf', 'ca']:\n",
    "    thinking_mean = df[(df['dataset'] == dataset) & (df['is_thinking'] == True)]['total_accuracy'].mean()\n",
    "    non_thinking_mean = df[(df['dataset'] == dataset) & (df['is_thinking'] == False)]['total_accuracy'].mean()\n",
    "    advantage = thinking_mean - non_thinking_mean\n",
    "    print(f\"   {dataset.upper()}: {advantage:+.1%} advantage for thinking models\")\n",
    "\n",
    "# Dataset difficulty\n",
    "print(f\"\\n5. DATASET DIFFICULTY:\")\n",
    "sf_mean = df[df['dataset'] == 'sf']['total_accuracy'].mean()\n",
    "ca_mean = df[df['dataset'] == 'ca']['total_accuracy'].mean()\n",
    "print(f\"   SF average: {sf_mean:.1%}\")\n",
    "print(f\"   CA average: {ca_mean:.1%}\")\n",
    "print(f\"   CA is {(ca_mean - sf_mean):.1%} easier on average\")\n",
    "\n",
    "print(f\"\\n6. INTERESTING FINDINGS:\")\n",
    "print(f\"    {len(df[df['is_thinking'] == True])} thinking models vs {len(df[df['is_thinking'] == False])} non-thinking models\")\n",
    "print(f\"    SF dataset is significantly more challenging than CA\")\n",
    "print(f\"    O-series models show strong performance despite being 'mini' versions\")\n",
    "print(f\"    Thinking capability provides measurable but variable improvements\")\n",
    "\n",
    "if not token_df.empty:\n",
    "    thinking_avg_tokens = token_df[token_df['is_thinking'] == True]['avg_tokens'].mean()\n",
    "    non_thinking_avg_tokens = token_df[token_df['is_thinking'] == False]['avg_tokens'].mean()\n",
    "    print(f\"    Thinking models use {thinking_avg_tokens/non_thinking_avg_tokens:.1f}x more tokens on average\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
